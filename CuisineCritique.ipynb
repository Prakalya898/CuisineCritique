{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7212e25",
   "metadata": {},
   "source": [
    "# Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a106685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd6b33",
   "metadata": {},
   "source": [
    "# Reading Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "review=pd.read_csv(\"E:\\\\DSC\\\\PROJECTS\\\\sentiment analysis\\\\Reviews.csv\")\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bdae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of entries in the data frame: ', review.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20123458",
   "metadata": {},
   "outputs": [],
   "source": [
    "review['ProductId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85943bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "review['UserId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a694f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c61f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6d46c",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261d3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(review, x=\"Score\")\n",
    "fig.update_traces(marker_color=\"orange\",marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=2.0)\n",
    "plt.figure(figsize=(5,3))\n",
    "fig.update_layout(title_text='Product Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555f99e",
   "metadata": {},
   "source": [
    "from this graph we can say that most of the reviews are postive.score is the ratings given by the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review[review['Score'] != 3]\n",
    "review['sentiment'] = review['Score'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "positive = review[review['sentiment'] == 1]\n",
    "negative = review[review['sentiment'] == -1]\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review['sentimentt'] = review['sentiment'].replace({-1 : 'negative'})\n",
    "review['sentimentt'] = review['sentimentt'].replace({1 : 'positive'})\n",
    "fig = px.histogram(review, x=\"sentimentt\")\n",
    "fig.update_traces(marker_color=\"indianred\",marker_line_color='rgb(10,48,107)',\n",
    "                  marker_line_width=1.5)\n",
    "fig.update_layout(title_text='Product Sentiment')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360a018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X=data['Score']\n",
    "y=data[\"sentiment\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.80, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss_train = StandardScaler()\n",
    "X_train = ss_train.fit_transform(X_train)\n",
    "ss_test = StandardScaler()\n",
    "X_test = ss_test.fit_transform(X_test)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(max_iter=10000).fit(X_train, y_train)\n",
    "\n",
    "svm_predict = svm.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, svm_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702fce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"E:\\\\DSC\\\\PROJECTS\\\\sentiment analysis\\\\Reviews.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=df\n",
    "label_encoders={}\n",
    "categorical_coloums= dt.columns\n",
    "for columns in categorical_coloums:\n",
    "    label_encoders[columns]=LabelEncoder()\n",
    "    dt[columns]=label_encoders[columns].fit_transform(dt[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f60bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cfac07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd63c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8eddce61",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ce294",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.drop(['ProductId','UserId','ProfileName','Id','HelpfulnessNumerator','HelpfulnessDenominator','Score','Time','Summary'], axis=1)\n",
    "review.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba939a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f93978",
   "metadata": {},
   "source": [
    "# Split the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(review['Text'], review['sentiment'], random_state = 0)\n",
    "print('X_train first entry: \\n\\n', X_train[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b335977",
   "metadata": {},
   "source": [
    "# Create a bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d73be",
   "metadata": {},
   "source": [
    "Next, we will use a count vectorizer from the Scikit-learn library.\n",
    "This will transform the text in our data frame into a bag of words model, which will contain a sparse matrix of integers. The number of occurrences of each word will be counted and printed.\n",
    "We will need to convert the text into a bag-of-words model since the logistic regression algorithm cannot understand text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(X_train)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d28c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the features\n",
    "feat = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8eca2",
   "metadata": {},
   "source": [
    "# Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "# the interpretation of the columns can be retreived as follows\n",
    "# X_train_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b98ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4996667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "predictions = model.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under the curve\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "print('AUC: ', roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237753cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('ROC for logistic regression on bag of words', fontsize=20)\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize = 20)\n",
    "plt.xlabel('False Positive Rate', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient determines the weight of a word (positivity or negativity)\n",
    "# checking the top 10 positive and negative words\n",
    "\n",
    "# getting the feature names\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# argsort: Integer indicies that would sort the index if used as an indexer\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "print('Smallest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-11:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975ad07",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultinomialNB()\n",
    "model.fit(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(vect.transform(X_test))\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "print('AUC: ', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2627769",
   "metadata": {},
   "source": [
    "It gives us a score of 0.844 which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40d7b1",
   "metadata": {},
   "source": [
    "# Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df = 5, ngram_range = (1,2)).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8546edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94666f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of features has increased again\n",
    "# checking for the AUC\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d605589",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "print('AUC: ', roc_auc)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('ROC for logistic regression on Bigrams', fontsize=20)\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize = 20)\n",
    "plt.xlabel('False Positive Rate', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c01481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the top 10 features for positive and negative\n",
    "# reviews again, the AUC has improved\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# print('Smallest Coef: \\n{}\\n'.format(feature_names[sorted_coef_index][:10]))\n",
    "# print('Largest Coef: \\n{}\\n'.format(feature_names[sorted_coef_index][:-11:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8de823",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = ['The food is not good, I would never buy them again']\n",
    "print(model.predict(vect.transform(new_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71980bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = ['One would not be disappointed by the food']\n",
    "print(model.predict(vect.transform(new_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c38be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
